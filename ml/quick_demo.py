#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ –æ—Ç–∑—ã–≤–æ–≤
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

def quick_demo_clustering(csv_file, sample_size=500, n_clusters=5):
    """
    –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –æ—Ç–∑—ã–≤–æ–≤
    
    Args:
        csv_file (str): –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –æ—Ç–∑—ã–≤–∞–º–∏
        sample_size (int): –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        n_clusters (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """
    print("üöÄ –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    
    print("üìñ –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ç–∑—ã–≤—ã...")
    df = pd.read_csv(csv_file, encoding='utf-8')
    
    if len(df) > sample_size:
        df_sample = df.sample(n=sample_size, random_state=42)
        print(f"‚úÖ –í–∑—è—Ç–∞ –≤—ã–±–æ—Ä–∫–∞ –∏–∑ {sample_size} –æ—Ç–∑—ã–≤–æ–≤ –∏–∑ {len(df)}")
    else:
        df_sample = df
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {len(df)} –æ—Ç–∑—ã–≤–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...")
    model = SentenceTransformer('cointegrated/rubert-tiny2')
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
    texts = df_sample['review_text'].fillna('').astype(str).tolist()
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    print("üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –æ—Ç–∑—ã–≤—ã...")
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(embeddings)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    df_sample = df_sample.copy()
    df_sample['cluster'] = cluster_labels
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
    print("üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã...")
    cluster_analysis = []
    
    for cluster_id in range(n_clusters):
        cluster_reviews = df_sample[df_sample['cluster'] == cluster_id]
        
        cluster_info = {
            'cluster_id': cluster_id,
            'size': len(cluster_reviews),
            'percentage': len(cluster_reviews) / len(df_sample) * 100,
            'avg_length': cluster_reviews['review_text'].str.len().mean(),
            'sample_reviews': cluster_reviews['review_text'].head(3).tolist()
        }
        cluster_analysis.append(cluster_info)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
    print("="*60)
    
    for info in sorted(cluster_analysis, key=lambda x: x['size'], reverse=True):
        print(f"\nüîπ –ö–ª–∞—Å—Ç–µ—Ä {info['cluster_id']} ({info['size']} –æ—Ç–∑—ã–≤–æ–≤, {info['percentage']:.1f}%)")
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {info['avg_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print("   –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤:")
        for i, sample in enumerate(info['sample_reviews'], 1):
            print(f"     {i}. {sample[:100]}{'...' if len(sample) > 100 else ''}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print("\nüîÑ –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é...")
    pca = PCA(n_components=2, random_state=42)
    reduced_embeddings = pca.fit_transform(embeddings)
    
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], 
                        c=cluster_labels, cmap='tab10', alpha=0.7, s=30)
    plt.colorbar(scatter)
    plt.title('–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –ø–æ —Ç–µ–º–∞–º (–¥–µ–º–æ)', fontsize=16)
    plt.xlabel('–ü–µ—Ä–≤–∞—è –≥–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞', fontsize=12)
    plt.ylabel('–í—Ç–æ—Ä–∞—è –≥–ª–∞–≤–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞', fontsize=12)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    for i in range(n_clusters):
        cluster_center = reduced_embeddings[cluster_labels == i].mean(axis=0)
        plt.annotate(f'–ö–ª–∞—Å—Ç–µ—Ä {i}', cluster_center, fontsize=12, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
    
    plt.savefig('/Users/nikitamesh/LCT/demo_clusters.png', dpi=200, bbox_inches='tight')
    plt.show()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
    df_sample.to_csv('/Users/nikitamesh/LCT/demo_clustered_reviews.csv', index=False, encoding='utf-8')
    
    analysis_df = pd.DataFrame(cluster_analysis)
    analysis_df.to_csv('/Users/nikitamesh/LCT/demo_cluster_analysis.csv', index=False, encoding='utf-8')
    
    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print("   - demo_clustered_reviews.csv - –æ—Ç–∑—ã–≤—ã —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏")
    print("   - demo_cluster_analysis.csv - –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    print("   - demo_clusters.png - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è")
    
    return df_sample, analysis_df

if __name__ == "__main__":
    df, analysis = quick_demo_clustering('/Users/nikitamesh/LCT/reviews_text_only.csv', 
                                       sample_size=300, n_clusters=5)
